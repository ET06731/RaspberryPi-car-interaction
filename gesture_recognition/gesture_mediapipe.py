#!/usr/bin/env python3
"""
æ‰‹åŠ¿è¯†åˆ« - MediaPipe Hands ç‰ˆæœ¬ (ç»ˆæç¨³å®šç‰ˆ)

MediaPipe Hands æä¾› 21 ä¸ªæ‰‹éƒ¨å…³é”®ç‚¹ï¼Œè¯†åˆ«å‡†ç¡®ç‡è¿œè¶…é¢œè‰²+è½®å»“æ–¹æ³•

æ‰‹åŠ¿å®šä¹‰:
- å¼ å¼€æ‰‹æŒ: æ‰€æœ‰æ‰‹æŒ‡ä¼¸ç›´
- æ¡æ‹³: æ‰€æœ‰æ‰‹æŒ‡å¼¯æ›²
- å‰ªåˆ€æ‰‹: é£ŸæŒ‡å’Œä¸­æŒ‡ä¼¸ç›´ï¼Œå…¶ä»–å¼¯æ›²
"""

import time
import threading
import cv2
import numpy as np
from collections import Counter

# å°è¯•å¯¼å…¥ MediaPipe
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("[OK] MediaPipe å·²åŠ è½½")
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    print("[ERROR] MediaPipe æœªå®‰è£…ï¼Œè¯·å…ˆè¿è¡Œ: pip3 install mediapipe --break-system-packages")
    exit(1)

# å¯¼å…¥ OLED æ˜¾ç¤º
try:
    from luma.core.interface.serial import i2c
    from luma.oled.device import ssd1306
    from PIL import Image, ImageDraw, ImageFont
    OLED_AVAILABLE = True
except ImportError:
    OLED_AVAILABLE = False
    print("[WARNING] OLED åº“æœªå®‰è£…")


class GestureSmoother:
    """æ—¶åºå¹³æ»‘å™¨"""
    def __init__(self, window_size=5, min_confidence=3):
        self.window_size = window_size
        self.min_confidence = min_confidence
        self.history = []
        self.stable_gesture = 'none'
    
    def update(self, raw_gesture):
        self.history.append(raw_gesture)
        if len(self.history) > self.window_size:
            self.history.pop(0)
        
        if len(self.history) < self.window_size:
            return None
        
        counter = Counter(self.history)
        most_common = counter.most_common(1)[0]
        gesture, count = most_common
        
        if count >= self.min_confidence:
            if gesture != self.stable_gesture:
                self.stable_gesture = gesture
                return gesture
        return None
    
    def get_stable(self):
        return self.stable_gesture


class MediaPipeGestureCamera:
    """åŸºäº MediaPipe çš„æ‰‹åŠ¿è¯†åˆ«"""
    
    def __init__(self, camera_index=0):
        self.cap = cv2.VideoCapture(camera_index)
        self.running = False
        self.current_gesture = 'none'
        self.lock = threading.Lock()
        self.frame = None
        
        # MediaPipe Hands åˆå§‹åŒ–
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=1,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
        )
        
        # æ‰‹æŒ‡å…³é”®ç‚¹ç´¢å¼•
        self.finger_tips = [8, 12, 16, 20]  # é£ŸæŒ‡ã€ä¸­æŒ‡ã€æ— åæŒ‡ã€å°æŒ‡å°–
        self.finger_pips = [6, 10, 14, 18]  # å¯¹åº”å…³èŠ‚
        self.thumb_tip = 4
        self.thumb_ip = 2
        
        if self.cap.isOpened():
            print("[OK] æ‘„åƒå¤´å·²è¿æ¥")
        else:
            print("[ERROR] æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
    
    def is_finger_extended(self, landmarks, tip_id, pip_id):
        """åˆ¤æ–­æ‰‹æŒ‡æ˜¯å¦ä¼¸ç›´"""
        tip = landmarks[tip_id]
        pip = landmarks[pip_id]
        wrist = landmarks[0]
        
        # è®¡ç®—åˆ°æ‰‹è…•çš„è·ç¦»
        tip_dist = ((tip.x - wrist.x) ** 2 + (tip.y - wrist.y) ** 2) ** 0.5
        pip_dist = ((pip.x - wrist.x) ** 2 + (pip.y - wrist.y) ** 2) ** 0.5
        
        return tip_dist > pip_dist
    
    def detect_gesture(self, frame):
        """ä½¿ç”¨ MediaPipe æ£€æµ‹æ‰‹åŠ¿"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)
        
        if results.multi_hand_landmarks:
            hand_landmarks = results.multi_hand_landmarks[0]
            landmarks = hand_landmarks.landmark
            
            # æ£€æµ‹å››æŒ‡çŠ¶æ€
            fingers = []
            for tip_id, pip_id in zip(self.finger_tips, self.finger_pips):
                fingers.append(self.is_finger_extended(landmarks, tip_id, pip_id))
            
            # æ‹‡æŒ‡ç‰¹æ®Šå¤„ç†ï¼ˆæ ¹æ® x åæ ‡åˆ¤æ–­ï¼‰
            thumb_extended = landmarks[self.thumb_tip].x < landmarks[self.thumb_ip].x
            
            extended_count = sum(fingers) + (1 if thumb_extended else 0)
            
            # æ‰‹åŠ¿åˆ†ç±»
            if extended_count == 5:
                return 'open_palm'
            elif extended_count == 0:
                return 'fist'
            elif extended_count == 2 and fingers[0] and fingers[1]:
                # åªæœ‰é£ŸæŒ‡å’Œä¸­æŒ‡ä¼¸ç›´
                return 'peace'
            elif extended_count == 1 and fingers[0]:
                # åªæœ‰é£ŸæŒ‡ä¼¸ç›´
                return 'point'
            
            return 'unknown'
        
        return 'none'
    
    def run(self):
        self.running = True
        print("[INFO] MediaPipe æ‰‹åŠ¿è¯†åˆ«å¯åŠ¨...")
        
        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                continue
            
            frame = cv2.flip(frame, 1)
            self.frame = frame.copy()
            
            gesture = self.detect_gesture(frame)
            
            with self.lock:
                self.current_gesture = gesture
        
        self.cap.release()
        self.hands.close()
        print("[OK] æ‘„åƒå¤´å·²å…³é—­")
    
    def get_gesture(self):
        with self.lock:
            return self.current_gesture
    
    def get_frame(self):
        return self.frame
    
    def draw_landmarks(self, frame):
        """åœ¨å¸§ä¸Šç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(rgb_frame)
        
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                self.mp_drawing.draw_landmarks(
                    frame,
                    hand_landmarks,
                    self.mp_hands.HAND_CONNECTIONS
                )
        
        return frame
    
    def stop(self):
        self.running = False


class OLEDDisplay:
    def __init__(self, port=1, address=0x3C):
        try:
            serial = i2c(port=port, address=address)
            self.device = ssd1306(serial, width=128, height=64)
            self.connected = True
            try:
                self.font_emoji = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 24)
                self.font_text = ImageFont.truetype("/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc", 14)
                self.font_small = ImageFont.truetype("/usr/share/fonts/opentype/noto/NotoSerifCJK-Bold.ttc", 11)
            except:
                self.font_emoji = ImageFont.load_default()
                self.font_text = ImageFont.load_default()
                self.font_small = ImageFont.load_default()
            print("[OK] OLED å·²è¿æ¥")
        except Exception as e:
            self.connected = False
            print(f"[ERROR] OLED è¿æ¥å¤±è´¥: {e}")
    
    def show(self, emoji, name, action):
        if not self.connected:
            return
        
        image = Image.new('1', (128, 64))
        draw = ImageDraw.Draw(image)
        draw.rectangle((0, 0, 127, 63), outline=0, fill=0)
        
        bbox = draw.textbbox((0, 0), emoji, font=self.font_emoji)
        x = (128 - (bbox[2] - bbox[0])) // 2
        draw.text((x, 0), emoji, font=self.font_emoji, fill=255)
        
        bbox = draw.textbbox((0, 0), name, font=self.font_text)
        x = (128 - (bbox[2] - bbox[0])) // 2
        draw.text((x, 30), name, font=self.font_text, fill=255)
        
        bbox = draw.textbbox((0, 0), action, font=self.font_small)
        x = (128 - (bbox[2] - bbox[0])) // 2
        draw.text((x, 48), action, font=self.font_small, fill=255)
        
        self.device.display(image)
    
    def clear(self):
        if self.connected:
            self.device.clear()


def main():
    print("=" * 60)
    print("  ğŸ¤š MediaPipe æ‰‹åŠ¿è¯†åˆ« (ç»ˆæç¨³å®šç‰ˆ)")
    print("=" * 60)
    print()
    print("ğŸš€ MediaPipe ä¼˜åŠ¿:")
    print("  âœ“ 21ä¸ªæ‰‹éƒ¨å…³é”®ç‚¹")
    print("  âœ“ ä¸å—å…‰ç…§/èƒŒæ™¯å½±å“")
    print("  âœ“ è¯†åˆ«ç‡ 95%+")
    print()
    print("æ”¯æŒæ‰‹åŠ¿:")
    print("  âœ‹ å¼ å¼€æ‰‹æŒ - (âœ§Ï‰âœ§)")
    print("  âœŠ æ¡æ‹³ - (â—£_â—¢)")
    print("  âœŒï¸ å‰ªåˆ€æ‰‹ - (â— â€¿â— )")
    print("  â˜ï¸ é£ŸæŒ‡æŒ‡ - (Â¬â€¿Â¬)")
    print()
    print("æŒ‰ Q é€€å‡ºï¼ŒæŒ‰ S æˆªå›¾")
    print()
    
    # æ‰‹åŠ¿æ˜ å°„è¡¨
    gestures = {
        'none': ('(-_-)', 'æœªæ£€æµ‹', 'è¯·å±•ç¤ºæ‰‹åŠ¿'),
        'unknown': ('(âŠ™_âŠ™)', 'æœªçŸ¥', 'å†è¯•ä¸€æ¬¡'),
        'open_palm': ('(âœ§Ï‰âœ§)', 'å¼ å¼€æ‰‹æŒ', 'ä½ å¥½ï¼'),
        'fist': ('(â—£_â—¢)', 'æ¡æ‹³', 'åŠ æ²¹ï¼'),
        'peace': ('(â— â€¿â— )', 'å‰ªåˆ€æ‰‹', 'è€¶ï¼'),
        'point': ('(Â¬â€¿Â¬)', 'æŒ‡å‘å‰', 'è¿™è¾¹ï¼'),
    }
    
    # åˆå§‹åŒ–
    camera = MediaPipeGestureCamera(0)
    oled = OLEDDisplay() if OLED_AVAILABLE else None
    smoother = GestureSmoother(window_size=5, min_confidence=3)
    
    if not camera.cap.isOpened():
        print("[ERROR] æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥")
        exit(1)
    
    camera_thread = threading.Thread(target=camera.run)
    camera_thread.start()
    
    current_display = 'none'
    screenshot_count = 0
    
    try:
        while True:
            raw_gesture = camera.get_gesture()
            stable_gesture = smoother.update(raw_gesture)
            
            # ç¨³å®šæ‰‹åŠ¿å˜åŒ–æ—¶æ›´æ–° OLED
            if stable_gesture is not None and stable_gesture != current_display:
                emoji, name, action = gestures.get(stable_gesture, gestures['unknown'])
                if oled:
                    oled.show(emoji, name, action)
                print(f"[è¯†åˆ«] {name} {emoji} - {action}")
                current_display = stable_gesture
            
            # æ˜¾ç¤ºè°ƒè¯•ç”»é¢
            frame = camera.get_frame()
            if frame is not None:
                # ç»˜åˆ¶ MediaPipe å…³é”®ç‚¹
                frame = camera.draw_landmarks(frame)
                
                # æ˜¾ç¤ºçŠ¶æ€
                stable = smoother.get_stable()
                cv2.putText(frame, f"Raw: {raw_gesture}", (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                cv2.putText(frame, f"Stable: {stable}", (10, 60), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                
                cv2.imshow("MediaPipe Gesture", frame)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s'):
                    filename = f"gesture_screenshot_{screenshot_count}.jpg"
                    cv2.imwrite(filename, frame)
                    print(f"[æˆªå›¾] å·²ä¿å­˜ {filename}")
                    screenshot_count += 1
            
            time.sleep(0.03)
    
    except KeyboardInterrupt:
        print("\né€€å‡ºä¸­...")
    
    finally:
        camera.stop()
        camera_thread.join()
        if oled:
            oled.clear()
        cv2.destroyAllWindows()
        print("[OK] å·²é€€å‡º")


if __name__ == "__main__":
    main()
